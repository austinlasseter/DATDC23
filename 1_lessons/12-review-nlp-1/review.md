# Review Topics


### Training set

### Testing set

### Overfitting

### Underfitting

When your model explains the relationship between inputs and outputs too poorly to explain out of sample examples. It has low bias, and, perhaps, low r-squared.

Look at metrics to determine -- score of the training data.

### Normalization

Process of transforming your data to a normal distribution. Mean of zero standard deviation of one.

Processing a data.

Many models assume normal data.

### Standardization

### Groupby

### Linear regression

### Logistic regression

### ROC-AUC

### Distribution

The "spread" of the data.

How data is spread. The range, mean, and deviation of data.

```python
import seaborn as sns
sns.distplot()
```

The AUC is area under the cruve. ROC is "weird name from a pilot guy."

Measuring sensitivity vs specificity.

TPR vs FPR are axes.

### False positive

Predict something is a yes, and it is actually a no. 

How well your model predicts those classes.

Using a confusion metrics in sklearn metrics.

### Lambda Functions

### Accuracy

### KNN

### Misclassification Rate

### Git workflow

### Mean squared error

### Mean absolute error

### BeautifulSoup

### KFolds

Take k-number of subsets of data. We do it to assess the quality of our model. Build a model for each of our subsets. We do this to make a better model.

We do this to simulate out of sample data more accurately.

Modelling and assessments.

Skikit learn -- model.selection KFolds().

### Unbalanced classes

### Describe

